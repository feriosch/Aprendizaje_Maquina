{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción al Aprendizaje No Supervisado\n",
    "\n",
    "Hasta ahora, tan sólo hemos explorado algoritmos y técnicas de Aprendizaje Automático supervisado para desarrollar modelos en los que los datos tenían etiquetas previamente conocidas. En otras palabras, nuestros datos tenían algunas variables objetivo con valores específicos que utilizamos para entrenar nuestros modelos.\n",
    "\n",
    "Sin embargo, cuando se trata de problemas del mundo real, la mayoría de las veces, los datos no vienen con etiquetas predefinidas, así que vamos a querer desarrollar modelos de aprendizaje automático que puedan clasificar correctamente estos datos, encontrando por sí mismos algunos puntos en común en las características, que se utilizarán para predecir las clases sobre nuevos datos.\n",
    "\n",
    "---\n",
    "\n",
    "## Proceso de Análisis del Aprendizaje no Supervisado\n",
    "\n",
    "El proceso general que seguiremos al desarrollar un modelo de aprendizaje no supervisado se puede resumir en el siguiente cuadro:\n",
    "\n",
    "![img](https://miro.medium.com/max/700/1*CLVgw3l1yDmtGVs8FMgyRw.png)\n",
    "\n",
    "Las principales aplicaciones de aprendizaje no supervisado son:\n",
    "\n",
    "* Segmentación de conjuntos de datos por atributos compartidos.\n",
    "* Detección de anomalías que no encajan en ningún grupo.\n",
    "* Simplificación de datasets agregando variables con atributos similares.\n",
    "\n",
    "En resumen, el objetivo principal es estudiar la estructura intrínseca (y comúnmente oculta) de los datos.\n",
    "\n",
    "---\n",
    "\n",
    "Estas técnicas se pueden condensar en dos tipos principales de problemas que el aprendizaje no supervisado trata de resolver. Estos son los problemas:\n",
    "\n",
    "* Agrupación\n",
    "* Reducción de la dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "En términos básicos, el objetivo de la agrupación es encontrar diferentes grupos dentro de los elementos de los datos. Para ello, los algoritmos de agrupamiento encuentran la estructura en los datos de manera que los elementos del mismo clúster (o grupo) sean más similares entre sí que con los de clústeres diferentes.\n",
    "\n",
    "De una manera visual: Imagina que tenemos un conjunto de datos de películas y queremos clasificarlas. Tenemos las siguientes reseñas de películas:\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*UrTFgcUrxq5C-wOUFvxCkQ.png)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "El modelo de aprendizaje automático podrá inferir que hay dos clases diferentes sin saber nada más de los datos.\n",
    "\n",
    "Estos algoritmos de aprendizaje no supervisados tienen una gama increíblemente amplia de aplicaciones y son muy útiles para resolver problemas del mundo real como la detección de anomalías, la recomendación de sistemas, la agrupación de documentos o la búsqueda de clientes con intereses comunes basados en sus compras.\n",
    "\n",
    "Algunos de los algoritmos de agrupación más comunes, y los que se explorarán a lo largo del artículo, son:\n",
    "\n",
    "* K-Medias\n",
    "* Clusterización Jerárquica\n",
    "* Density Based Scan Clustering (DBSCAN)\n",
    "* Modelo de Agrupamiento Gaussiano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación K-Meedias\n",
    "\n",
    "Los algoritmos de K-Medias son extremadamente fáciles de implementar y muy eficientes desde el punto de vista computacional. Estas son las principales razones que explican por qué son tan populares. Pero no son muy buenos para identificar clases cuando se trata de grupos que no tienen una forma de distribución esférica.\n",
    "\n",
    "El algoritmo K-Means tiene como objetivo encontrar y agrupar en clases los puntos de datos que tienen una alta similitud entre ellos. En los términos del algoritmo, esta similitud se entiende como lo opuesto de la distancia entre puntos de datos. Cuanto más cerca estén los puntos de datos, más similares y con más probabilidades de pertenecer al mismo clúster serán.\n",
    "\n",
    "---\n",
    "\n",
    "#### Conceptos clave\n",
    "\n",
    "* Distancia Cuadrada Euclidiana\n",
    "\n",
    "La distancia más comúnmente utilizada en K-Means es la distancia cuadrada de Euclides. Un ejemplo de esta distancia entre dos puntos x e y en el espacio m-dimensional es:\n",
    "![img](https://miro.medium.com/proxy/1*svzWIVVO4k0tSu14pzSuFA.png)\n",
    "\n",
    "Aquí, j es la dimensión j (o columna de características) de los puntos de muestra x e y.\n",
    "\n",
    "* Inercia de los Clústeres\n",
    "\n",
    "La inercia del cluster es el nombre dado a la Suma de Errores Cuadrados dentro del contexto del clustering, y se representa de la siguiente manera:\n",
    "![img](https://miro.medium.com/proxy/1*jO8AEM1Ttkc46ea7bIEl0Q.png)\n",
    "\n",
    "---\n",
    "\n",
    "Donde $μ(j)$ es el centroide del cluster $j$, y $w(i,j)$ es 1 si la muestra $x(i)$ está en el cluster $j$ y 0 en caso contrario.\n",
    "\n",
    "K-Means puede ser entendido como un algoritmo que intentará minimizar el factor de inercia del cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasos del Algoritmo\n",
    "\n",
    "1. Primero, necesitamos elegir k, el número de clusters que queremos que nos encuentren.\n",
    "2. Luego, el algoritmo seleccionará aleatoriamente los centroides de cada grupo.\n",
    "3. Se asignará cada punto de datos al centroide más cercano (utilizando la distancia euclídea).\n",
    "4. Se calculará la inercia del conglomerado.\n",
    "5. Los nuevos centroides se calcularán como la media de los puntos que pertenecen al centroide del paso anterior. En otras palabras, calculando el error cuadrático mínimo de los puntos de datos al centro de cada cluster, moviendo el centro hacia ese punto.\n",
    "6. Volver al paso 3.\n",
    "\n",
    "### Hiperparámetros de K-Means\n",
    "\n",
    "* Número de grupos: El número de clusters y centros de generación.\n",
    "* Máximas iteraciones: del algoritmo para una sola ejecución.\n",
    "* Número inicial: El número de veces que el algoritmo se ejecutará con diferentes semillas de centroide. El resultado final será el mejor rendimiento del número definido de corridas consecutivas, en términos de inercia.\n",
    "\n",
    "### Los Retos de K-Medias\n",
    "\n",
    "* El resultado de cualquier set de entrenamiento fijo no siempre será el mismo, porque los centroides iniciales se fijan al azar y eso influirá en todo el proceso del algoritmo.\n",
    "* Como ya se ha dicho, debido a la naturaleza de la distancia euclídea, no es un algoritmo adecuado cuando se trata de clusters que adoptan formas no esféricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Puntos a Tener en Cuenta al Aplicar K-Means\n",
    "\n",
    "Las características deben medirse en la misma escala, por lo que puede ser necesario realizar la estandarización de la escala z-score o la escala max-min. Cuando se trate de datos categóricos, utilizaremos la función get dummies.\n",
    "\n",
    "El Análisis Exploratorio de Datos (EDA) es muy útil para tener una visión general de los datos y determinar si K-Medias es el algoritmo más apropiado. Y el método de minibatch es muy útil cuando hay un gran número de columnas, sin embargo, es menos preciso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cómo elegir el número K correcto?\n",
    "\n",
    "La elección del número correcto de clusters es uno de los puntos clave del algoritmo K-Means. Para encontrar este número hay algunos métodos:\n",
    "\n",
    "* Conocimiento del campo\n",
    "* Decisión de negocios\n",
    "* Método del codo\n",
    "\n",
    "Al estar alineado con la motivación y la naturaleza de la ciencia de datos, el método del codo es la opción preferida, ya que se basa en un método analítico respaldado con datos, para tomar una decisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método del codo\n",
    "\n",
    "El método del codo se utiliza para determinar el número correcto de grupos en un dataset. Funciona trazando los valores ascendentes de K frente al error total obtenido al usar esa K.\n",
    "![img](https://miro.medium.com/max/453/1*9115ruKQGOa4qa4xGZzL_A.png)\n",
    "\n",
    "El objetivo es encontrar la k adecuada para que en cada cluster no aumente significativamente la varianza\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*86R1OByRi6JoLq1JPAUnpQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitaciones de K-Medias\n",
    "\n",
    "Aunque K-Medias es un gran algoritmo de agrupación, es más útil cuando sabemos de antemano el número exacto de grupos y cuando estamos tratando con distribuciones esféricas.\n",
    "\n",
    "La siguiente imagen muestra lo que obtendríamos si utilizáramos la agrupación de medios K en cada conjunto de datos, incluso si conociéramos de antemano el número exacto de grupos:\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*ykyaNxEi1QhICv8gbdI8aw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación Jerárquica\n",
    "\n",
    "La agrupación jerárquica es una alternativa a los algoritmos de agrupación basados en prototipos. La principal ventaja de la agrupación jerárquica es que no necesitamos especificar el número de agrupaciones, la encontrará por sí misma. Además, permite el trazado de dendogramas. Los dendogramas son visualizaciones de una agrupación jerárquica binaria.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*GDuQNu0Ioz0cuUgwnvCPGg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las observaciones que se fusionan en la parte inferior son similares, mientras que las que están en la parte superior son muy diferentes. Con los dendogramas, las conclusiones se hacen basándose en la ubicación del eje vertical y no en el horizontal.\n",
    "\n",
    "\n",
    "### Tipos de Agrupación Jerárquica\n",
    "\n",
    "Existen dos enfoques para este tipo de agrupación: Aglomerantio y divisivo.\n",
    "\n",
    "* Divisivo: este método comienza por englobar todos los puntos de datos en un solo grupo. Luego, dividirá el grupo iterativamente en otros mas pequeños hasta que cada uno de ellos contenga sólo una muestra.\n",
    "* Aglomerativo: este método comienza con cada muestra siendo un grupo diferente y luego fusionándolas por las que están más cerca unas de otras hasta que sólo haya un grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acoplamientos Único y Completo\n",
    "\n",
    "Estos son los algoritmos más comunes utilizados para la agrupación jerárquica aglomerativa.\n",
    "![img](https://miro.medium.com/max/358/1*mVj9fkikBpGyJ0xME46uMg.png)\n",
    "\n",
    "---\n",
    "\n",
    "* Acoplamiento Simple\n",
    "\n",
    "Al ser un algoritmo aglomerativo, el enlace simple comienza asumiendo que cada punto de la muestra es un conglomerado. Luego, calcula las distancias entre los miembros más similares para cada par de cúmulos y fusiona los dos cúmulos para los cuales la distancia entre los miembros más similares es la más pequeña.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*HUOYokgnLlokcYvT2C4stg.png)\n",
    "\n",
    "* Acoplamiento Completo\n",
    "\n",
    "Aunque es similar a su hermano (single linkage) su filosofía es exactamente la opuesta, compara los puntos de datos más diferentes de un par de grupos para realizar la fusión.\n",
    "\n",
    "---\n",
    "\n",
    "#### Ventajas de la Agrupación Jerárquica\n",
    "\n",
    "    Las representaciones jerárquicas resultantes pueden ser muy informativas.\n",
    "    Los dendogramas proporcionan una forma interesante e informativa de visualización.\n",
    "    Son especialmente potentes cuando el conjunto de datos contiene relaciones jerárquicas reales.\n",
    "\n",
    "#### Desventajas de la Agrupación Jerárquica\n",
    "\n",
    "    Son muy sensibles a los valores atípicos y, en su presencia, el rendimiento del modelo disminuye significativamente.\n",
    "    Son muy exigentes, desde el punto de vista informático y computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupación Espacial de Aplicaciones Basadas en la Densidad con Ruido (DBSCAN)\n",
    "\n",
    "La Agrupación Espacial Basada en Densidad de Aplicaciones con Ruido, o DBSCAN (Density-Based Spatial Clustering of Applications with Noise), es otro algoritmo de agrupación especialmente útil para identificar correctamente el ruido en los datos.\n",
    "\n",
    "---\n",
    "\n",
    "### Criterios de Asignación de DBSCAN\n",
    "\n",
    "Se basa en un número de puntos con un radio especificado ε y hay una etiqueta especial asignada a cada punto de datos. El proceso de asignación de esta etiqueta es el siguiente:\n",
    "\n",
    "    Es un número especificado (MinPts) de puntos vecinos. Se asignará un punto central si existe este número de puntos de los MinPts que caen en el radio ε\n",
    "    Un punto fronterizo caerá en el radio de ε de un punto central, pero tendrá menos vecinos que el número de MinPts.\n",
    "    Todos los demás puntos serán puntos de ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo DBSCAN\n",
    "\n",
    "El algoritmo sigue la lógica:\n",
    "\n",
    "* Identificar un punto central y hacer un grupo para cada uno, o para cada grupo conectado de puntos centrales (si es que establecen que el criterio es el punto central).\n",
    "* Identificar y asignar puntos fronterizos a sus respectivos puntos centrales.\n",
    "\n",
    "La siguiente figura resume muy bien este proceso y la notación comentada.\n",
    "DBSCAN vs. K-Means Clustering\n",
    "![img](https://miro.medium.com/proxy/1*USv6WLj3A-9De9D7am2iZQ.png)\n",
    "\n",
    "---\n",
    "![img](https://miro.medium.com/proxy/1*x48iVUvrWtYY31WEsVLdeQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ventajas de DBDSCAN\n",
    "\n",
    "    No es necesario especificar el número de grupos.\n",
    "    Existe una gran flexibilidad en las formas y tamaños que pueden adoptar los grupos.\n",
    "    Es muy útil para identificar y tratar con datos de ruido y valores atípicos.\n",
    "\n",
    "### Desventajas de DBSCAN\n",
    "\n",
    "    Se enfrenta a dificultades a la hora de tratar los puntos de encuentro que son alcanzables por dos grupos.\n",
    "    No encuentra bien racimos de densidades variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Mezcla Gaussiana (MMG)\n",
    "\n",
    "Los Modelos de Mezcla Gaussiana son modelos probabilísticos que asumen que todas las muestras son generadas a partir de una mezcla de un número de finita de distribución gaussiana con parámetros desconocidos.\n",
    "\n",
    "Pertenece al grupo de algoritmos de agrupamiento blando en el que cada punto de datos pertenecerá a cada grupo existente en el conjunto de datos, pero con diferentes niveles de pertenencia a cada grupo. Esta membresía se asigna como la probabilidad de pertenecer a un determinado grupo, que oscila entre 0 y 1.\n",
    "\n",
    "Por ejemplo, el punto resaltado pertenecerá a los grupos A y B simultáneamente, pero con mayor pertenencia al grupo A, debido a su cercanía.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*jCGgXVlHGE3cXVncW3xdtw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GMM es uno de los métodos de agrupación más avanzados que estudiaremos en esta serie, asume que cada grupo sigue una distribución probabilística que puede ser Gaussiana o Normal. Es una generalización de la agrupación de K-Means que incluye información sobre la estructura de covarianza de los datos así como los centros de los gaussianos latentes.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*1QDCWZS0AUZ-51VKG8INiQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de MMG en una dimensión\n",
    "\n",
    "El MMG buscará distribuciones gaussianas en el conjunto de datos y las mezclará.\n",
    "![img](https://miro.medium.com/proxy/1*uc63ZNYZaVcW75QOcJyAtQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMG en dos dimensiones\n",
    "\n",
    "Cuando se tienen distribuciones multivariantes como la siguiente, el centro medio sería µ + σ, para cada eje de la distribución del conjunto de datos.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*wkTgfCOdSS06ia6KJDAENw.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo del MMG\n",
    "\n",
    "Es un algoritmo de maximización de expectativas cuyo proceso podría resumirse de la siguiente manera:\n",
    "\n",
    "    Inicializar las distribuciones de K Gaussian. Lo hace con los valores µ (media) y σ (desviación estándar). Se pueden tomar del conjunto de datos (método ingenuo) o aplicando K-Medias.\n",
    "    Grupo ‘blando’ de datos: es la fase de’Expectativa’ en la que todos los puntos de datos se asignarán a cada grupo con su respectivo nivel de membresía.\n",
    "    Reestimar a los gaussianos: es la fase de ‘Maximización’ en la que se comprueban las expectativas y se utilizan para calcular nuevos parámetros para los gaussianos: nuevos µ y σ.\n",
    "    Evaluar la probabilidad de registro de los datos para verificar la convergencia. Cuanto mayor sea la similitud con el registro, más probable es que la mezcla del modelo que creamos se ajuste a nuestro conjunto de datos. Por lo tanto, esta es la función de maximizar.\n",
    "    Repetir desde el paso 2 hasta la convergencia.\n",
    "    \n",
    "---\n",
    "\n",
    "### Ventajas del MMG\n",
    "\n",
    "    Es un método de agrupación en grupos blandos, que asigna etiquetas de pertenencia a varios grupos. Esta característica lo convierte en el algoritmo más rápido para aprender modelos de mezclas.\n",
    "    Existe una gran flexibilidad en el número y la forma de los grupos.\n",
    "\n",
    "### Desventajas del MMG\n",
    "\n",
    "    Es muy sensible a los valores iniciales que condicionarán en gran medida su rendimiento.\n",
    "    El MMG puede converger a un mínimo local, lo que constituiría una solución no óptima.\n",
    "    Al tener puntos insuficientes por mezcla, el algoritmo diverge y encuentra soluciones con infinitas probabilidades a menos que regularicemos artificialmente las covarianzas entre los puntos de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validación de Agrupación\n",
    "\n",
    "La validación de agrupación es el proceso de evaluación objetiva y cuantitativa del resultado de un grupo. Haremos esta validación aplicando índices de validación de grupos. Hay tres categorías principales:\n",
    "\n",
    "### Índices Externos\n",
    "\n",
    "Se trata de métodos de puntuación que utilizamos si se etiquetan los datos originales, lo que no es el caso más frecuente en este tipo de problemas. Emparejaremos una estructura de grupo con la información conocida de antemano.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*MMtZnLHmzEYmF5K7zfMbHA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El índice más utilizado es el índice de rand ajustado.\n",
    "\n",
    "* Índice de Rand Ajustado $(ARI) €[-1,1]$\n",
    "\n",
    "Para entenderlo, primero debemos definir sus componentes:\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*n5GidEL8cG-zzhyXQNWqCQ.png)\n",
    "\n",
    "$a$ es el número de puntos que están en el mismo cluster tanto en C como en K\n",
    "\n",
    "$b$ es el número de puntos que se encuentran en los diferentes clusters tanto en C como en K.\n",
    "\n",
    "$n$ = es el número total de muestras\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*tSJ0NX7ZHwqpsRVhQrSebA.png)\n",
    "\n",
    "El ARI puede obtener valores que van de -1 a 1. Cuanto más alto sea el valor, mejor se ajusta a los datos originales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Índices de Validación Interna\n",
    "\n",
    "En el aprendizaje no supervisado, trabajaremos con datos no etiquetados y es entonces cuando los índices internos son más útiles.\n",
    "\n",
    "Uno de los índices más comunes es el Coeficiente de Silueta.\n",
    "\n",
    "* Coeficiente de silueta:\n",
    "\n",
    "Hay un Coeficiente de Silueta para cada punto de datos.\n",
    "\n",
    "![img](https://miro.medium.com/proxy/1*qydd2In0-dD7jo8yXfGVuQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Hemos hecho una primera introducción al aprendizaje no supervisado y a los principales algoritmos de clustering.\n",
    "\n",
    "Posteriormente repasaremos una implementación que servirá como ejemplo para construir un modelo K-means y revisaremos y pondremos en práctica los conceptos explicados."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
